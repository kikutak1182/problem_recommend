# ONNX Sentence Embedding API

é«˜æ€§èƒ½ãƒ»è»½é‡ãªæ—¥æœ¬èªå¯¾å¿œæ–‡ç« åŸ‹ã‚è¾¼ã¿APIã€‚PyTorchã«ä¾å­˜ã›ãšã€ONNX Runtime + INT8é‡å­åŒ–ã§å¤§å¹…ãªè»½é‡åŒ–ã‚’å®Ÿç¾ã€‚

## ğŸ¯ ç›®çš„ã¨å…¨ä½“åƒ

- **ãƒ¢ãƒ‡ãƒ«**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **å¤‰æ›**: Sentence-Transformers â†’ ONNX â†’ INT8é‡å­åŒ–
- **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **: PyTorchéä¾å­˜ï¼ˆ`onnxruntime` + `tokenizers`ã®ã¿ï¼‰
- **ã‚µã‚¤ã‚º**: ~100-200MBï¼ˆvs 1.7GB PyTorchç‰ˆï¼‰
- **äº’æ›æ€§**: Sentence-Transformersã¨åŒç­‰ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å‡ºåŠ›

## ğŸ› ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¤‰æ›ç”¨ï¼‰

```bash
# PyTorchã¨Transformersã¯å¤‰æ›æ™‚ã®ã¿å¿…è¦
pip install torch transformers sentence-transformers onnx onnxruntime onnxruntime-tools
```

### 2. ONNXå¤‰æ›ã®å®Ÿè¡Œ

```bash
# Step 1: Sentence-Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›
python export_onnx.py

# Step 2: INT8é‡å­åŒ–ï¼ˆã‚µã‚¤ã‚ºå‰Šæ¸›ï¼‰
python quantize.py
```

**ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«**:
- `model_fp32.onnx` - FP32 ONNXãƒ¢ãƒ‡ãƒ«
- `model_int8.onnx` - INT8é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆæœ¬ç•ªç”¨ï¼‰
- `tokenizer/` - Tokenizerãƒ•ã‚¡ã‚¤ãƒ«

## ğŸ³ Dockerå®Ÿè¡Œ

### ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ

```bash
# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ“ãƒ«ãƒ‰
docker build -t onnx-embedding-api .

# ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•
docker run -d \
    --name embedding-api \
    -e PORT=8080 \
    -p 8080:8080 \
    onnx-embedding-api

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8080/healthz
```

### APIãƒ†ã‚¹ãƒˆ

```bash
# å˜ç™ºåŸ‹ã‚è¾¼ã¿
curl "http://localhost:8080/embed?q=ãƒ¯ãƒ¼ã‚·ãƒ£ãƒ«ãƒ•ãƒ­ã‚¤ãƒ‰"

# ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿
curl -X POST "http://localhost:8080/embed" \
    -H "Content-Type: application/json" \
    -d '{"texts": ["å‹•çš„è¨ˆç”»æ³•", "ã‚°ãƒ©ãƒ•ç†è«–", "æ·±ã•å„ªå…ˆæ¢ç´¢"]}'

# APIæƒ…å ±
curl http://localhost:8080/
```

## â˜ï¸ Cloud Run ãƒ‡ãƒ—ãƒ­ã‚¤

### 1. Artifact Registry ãƒªãƒã‚¸ãƒˆãƒªä½œæˆ

```bash
# GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
export PROJECT_ID=your-project-id
export REGION=asia-northeast1
export REPO_NAME=onnx-embedding

# Artifact Registry ãƒªãƒã‚¸ãƒˆãƒªä½œæˆ
gcloud artifacts repositories create $REPO_NAME \
    --repository-format=docker \
    --location=$REGION \
    --project=$PROJECT_ID
```

### 2. ã‚³ãƒ³ãƒ†ãƒŠç™»éŒ²

```bash
# Dockerèªè¨¼è¨­å®š
gcloud auth configure-docker ${REGION}-docker.pkg.dev

# ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ã‚¿ã‚°ä»˜ã‘
docker tag onnx-embedding-api \
    ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/embedding-api:latest

# ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ãƒ—ãƒƒã‚·ãƒ¥
docker push ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/embedding-api:latest
```

### 3. Cloud Run ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
# Cloud Runã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‡ãƒ—ãƒ­ã‚¤
gcloud run deploy embedding-api \
    --image ${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/embedding-api:latest \
    --platform managed \
    --region $REGION \
    --allow-unauthenticated \
    --memory 1Gi \
    --cpu 1 \
    --timeout 300s \
    --concurrency 10 \
    --max-instances 100
```

**ã¾ãŸã¯ Google Cloud Console ã§ã®æ“ä½œ**:
1. Cloud Run â†’ ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
2. ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸URL: `${REGION}-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/embedding-api:latest`
3. ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: `asia-northeast1`
4. èªè¨¼: æœªèªè¨¼ã®å‘¼ã³å‡ºã—ã‚’è¨±å¯ï¼ˆä»»æ„ï¼‰
5. ãƒ¡ãƒ¢ãƒª: 1-2 GiB
6. CPU: 1
7. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 300ç§’

## ğŸ“Š APIä»•æ§˜

### ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ |
|---------------|---------|------|
| `/healthz` | GET | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ |
| `/embed` | POST | ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ |
| `/embed?q=<text>` | GET | å˜ç™ºåŸ‹ã‚è¾¼ã¿ |
| `/docs` | GET | APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ |

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
// GET /embed?q=å‹•çš„è¨ˆç”»æ³•
{
    "vector": [0.1, -0.2, 0.3, ...],  // 384æ¬¡å…ƒ
    "dimension": 384,
    "text": "å‹•çš„è¨ˆç”»æ³•"
}

// POST /embed
{
    "vectors": [[0.1, -0.2, ...], [0.3, 0.4, ...]],
    "dimension": 384,
    "count": 2
}
```

## âš™ï¸ ç’°å¢ƒå¤‰æ•°

| å¤‰æ•° | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------|-----------|------|
| `MODEL_PATH` | `model_int8.onnx` | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ |
| `TOKENIZER_PATH` | `tokenizer/` | Tokenizerãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª |
| `MAX_LENGTH` | `256` | æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•· |
| `PORT` | `8080` | APIãƒãƒ¼ãƒˆ |

## ğŸ”§ ç²¾åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### INT8é‡å­åŒ–ã®å½±éŸ¿

- **ã‚µã‚¤ã‚ºå‰Šæ¸›**: ç´„75-80%ï¼ˆä¾‹: 400MB â†’ 100MBï¼‰
- **æ¨è«–é€Ÿåº¦**: 2-3å€é«˜é€ŸåŒ–
- **ç²¾åº¦ä½ä¸‹**: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§å¹³å‡ < 0.02ã®èª¤å·®
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§å¹…å‰Šæ¸›ï¼ˆ1/4ç¨‹åº¦ï¼‰

### FP32ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ï¼ˆé«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆï¼‰

```bash
# FP32ãƒ¢ãƒ‡ãƒ«ã§èµ·å‹•
docker run -e MODEL_PATH=model_fp32.onnx -p 8080:8080 onnx-embedding-api
```

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¾‹

```python
# é‡å­åŒ–å“è³ªãƒ†ã‚¹ãƒˆï¼ˆä¾‹ï¼‰
texts = ["å‹•çš„è¨ˆç”»æ³•", "ã‚°ãƒ©ãƒ•ç†è«–", "æ·±ã•å„ªå…ˆæ¢ç´¢"]
fp32_embeddings = embed_fp32(texts)
int8_embeddings = embed_int8(texts)

# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å·®åˆ†
for i, text in enumerate(texts):
    similarity = cosine_similarity([fp32_embeddings[i]], [int8_embeddings[i]])[0][0]
    print(f"{text}: {similarity:.4f}")  # é€šå¸¸ > 0.98
```

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

**1. `tokenizer.json ãŒèª­ã‚ãªã„`**
```
FileNotFoundError: Tokenizer file not found: tokenizer/tokenizer.json
```
â†’ `python export_onnx.py`ã‚’å®Ÿè¡Œã—ã¦tokenizerã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

**2. `CLS/SEP ãƒˆãƒ¼ã‚¯ãƒ³IDãŒä¸æ­£`**
```
è­¦å‘Š: CLS/SEP token IDs may be incorrect
```
â†’ ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚é€šå¸¸ã¯ CLS:101, SEP:102, PAD:0

**3. `å‹•çš„è»¸ã‚¨ãƒ©ãƒ¼`**
```
ONNX Runtime error: Invalid input shape
```
â†’ `export_onnx.py`ã§dynamic_axesãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

**4. `PORTæœªè¨­å®šã‚¨ãƒ©ãƒ¼`**
```
uvicorn: PORT environment variable not set
```
â†’ `export PORT=8080`ã‚’è¨­å®šã™ã‚‹ã‹ã€`-e PORT=8080`ã§ã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ã¦ãã ã•ã„ã€‚

**5. `ãƒ¡ãƒ¢ãƒªä¸è¶³`**
```
Container killed due to memory limit
```
â†’ Cloud Runã®ãƒ¡ãƒ¢ãƒªã‚’1-2GiBã«å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚

### ãƒ‡ãƒãƒƒã‚°ç”¨ã‚³ãƒãƒ³ãƒ‰

```bash
# ã‚³ãƒ³ãƒ†ãƒŠãƒ­ã‚°ã®ç¢ºèª
docker logs embedding-api

# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®ç¢ºèª
docker exec -it embedding-api /bin/bash
ls -la
python -c "import onnxruntime; print(onnxruntime.__version__)"

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
python -c "
import onnx
model = onnx.load('model_int8.onnx')
print(f'Input: {[inp.name for inp in model.graph.input]}')
print(f'Output: {[out.name for out in model.graph.output]}')
"
```

## ğŸ“ å—ã‘å…¥ã‚Œæ¡ä»¶ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] `python export_onnx.py`ã§`tokenizer/`ã¨`model_fp32.onnx`ãŒç”Ÿæˆã•ã‚Œã‚‹
- [ ] `python quantize.py`ã§`model_int8.onnx`ãŒç”Ÿæˆã•ã‚Œã‚‹
- [ ] `docker run`ã§èµ·å‹•ã—ã€`GET /healthz`ãŒ`{"ok": true}`ã‚’è¿”ã™
- [ ] `GET /embed?q=ãƒ™ãƒ«ãƒãƒ³ãƒ•ã‚©ãƒ¼ãƒ‰`ãŒ384æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™
- [ ] é‡å­åŒ–ç‰ˆã¨éé‡å­åŒ–ç‰ˆã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦å·®åˆ†ãŒå¹³å‡ < 0.02
- [ ] ã‚³ãƒ³ãƒ†ãƒŠã‚µã‚¤ã‚ºãŒ100-200MBå°
- [ ] Cloud Runã§æ­£å¸¸å‹•ä½œ

## ğŸ—ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
app/
â”œâ”€â”€ export_onnx.py          # ONNXå¤‰æ›ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼‰
â”œâ”€â”€ quantize.py             # INT8é‡å­åŒ–
â”œâ”€â”€ main.py                 # FastAPIæ¨è«–ã‚µãƒ¼ãƒãƒ¼
â”œâ”€â”€ requirements.txt        # ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ä¾å­˜é–¢ä¿‚
â”œâ”€â”€ Dockerfile              # Cloud Runç”¨
â”œâ”€â”€ .dockerignore          # Dockeré™¤å¤–è¨­å®š
â”œâ”€â”€ README.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ tokenizer/             # Tokenizerãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç”Ÿæˆï¼‰
â”œâ”€â”€ model_fp32.onnx        # FP32ãƒ¢ãƒ‡ãƒ«ï¼ˆç”Ÿæˆï¼‰
â””â”€â”€ model_int8.onnx        # INT8ãƒ¢ãƒ‡ãƒ«ï¼ˆç”Ÿæˆãƒ»æœ¬ç•ªç”¨ï¼‰
```

## ğŸ“š å‚è€ƒæƒ…å ±

- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Tokenizers Library](https://huggingface.co/docs/tokenizers/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Google Cloud Run Documentation](https://cloud.google.com/run/docs)